{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "#from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari_Wrapper(gym.Wrapper):\n",
    "    # env wrapper to resize images, grey scale and frame stacking and other misc.\n",
    "    \n",
    "    def __init__(self, env, env_name, k, dsize=(84,84), use_add_done=False):\n",
    "        super(Atari_Wrapper, self).__init__(env)\n",
    "        self.dsize = dsize\n",
    "        self.k = k\n",
    "        self.use_add_done = use_add_done\n",
    "        \n",
    "        # set image cutout depending on game\n",
    "        if \"Pong\" in env_name:\n",
    "            self.frame_cutout_h = (33,-15)\n",
    "            self.frame_cutout_w = (0,-1)\n",
    "        elif \"Breakout\" in env_name:\n",
    "            self.frame_cutout_h = (31,-16)\n",
    "            self.frame_cutout_w = (7,-7)\n",
    "        elif \"SpaceInvaders\" in env_name:\n",
    "            self.frame_cutout_h = (25,-7)\n",
    "            self.frame_cutout_w = (7,-7)\n",
    "        else:\n",
    "            # no cutout\n",
    "            self.frame_cutout_h = (0,-1)\n",
    "            self.frame_cutout_w = (0,-1)\n",
    "        \n",
    "    def reset(self):\n",
    "    \n",
    "        self.Return = 0\n",
    "        self.last_life_count = 0\n",
    "        \n",
    "        ob = self.env.reset()[0]\n",
    "        ob = self.preprocess_observation(ob)\n",
    "        \n",
    "        # stack k times the reset ob\n",
    "        self.frame_stack = np.stack([ob for i in range(self.k)])\n",
    "        \n",
    "        return self.frame_stack\n",
    "    \n",
    "    \n",
    "    def step(self, action): \n",
    "        # do k frameskips, same action for every intermediate frame\n",
    "        # stacking k frames\n",
    "        \n",
    "        reward = 0\n",
    "        done = False\n",
    "        additional_done = False\n",
    "        \n",
    "        # k frame skips or end of episode\n",
    "        frames = []\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            ob, r, d, _,info = self.env.step(action)\n",
    "            \n",
    "            # insert a (additional) done, when agent loses a life (Games with lives)\n",
    "            if self.use_add_done:\n",
    "                if info['lives'] < self.last_life_count:\n",
    "                    additional_done = True  \n",
    "                self.last_life_count = info['lives']\n",
    "            \n",
    "            ob = self.preprocess_observation(ob)\n",
    "            frames.append(ob)\n",
    "            \n",
    "            # add reward\n",
    "            reward += r\n",
    "            \n",
    "            if d: # env done\n",
    "                done = True\n",
    "                break\n",
    "                       \n",
    "        # build the observation\n",
    "        self.step_frame_stack(frames)\n",
    "        \n",
    "        # add info, get return of the completed episode\n",
    "        self.Return += reward\n",
    "        if done:\n",
    "            info[\"return\"] = self.Return\n",
    "            \n",
    "        # clip reward\n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward == 0:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        return self.frame_stack, reward, done, info, additional_done\n",
    "    \n",
    "    def step_frame_stack(self, frames):\n",
    "        \n",
    "        num_frames = len(frames)\n",
    "        \n",
    "        if num_frames == self.k:\n",
    "            self.frame_stack = np.stack(frames)\n",
    "        elif num_frames > self.k:\n",
    "            self.frame_stack = np.array(frames[-k::])\n",
    "        else: # mostly used when episode ends \n",
    "            \n",
    "            # shift the existing frames in the framestack to the front=0 (0->k, index is time)\n",
    "            self.frame_stack[0: self.k - num_frames] = self.frame_stack[num_frames::]\n",
    "            # insert the new frames into the stack\n",
    "            self.frame_stack[self.k - num_frames::] = np.array(frames)  \n",
    "            \n",
    "    def preprocess_observation(self, ob):\n",
    "    # resize and grey and cutout image\n",
    "    \n",
    "        ob = cv2.cvtColor(ob[self.frame_cutout_h[0]:self.frame_cutout_h[1],\n",
    "                           self.frame_cutout_w[0]:self.frame_cutout_w[1]], cv2.COLOR_BGR2GRAY)\n",
    "        ob = cv2.resize(ob, dsize=self.dsize)\n",
    "    \n",
    "        return ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    # nature paper architecture\n",
    "    \n",
    "    def __init__(self, in_channels, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        network = [\n",
    "            torch.nn.Conv2d(in_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        ]\n",
    "        \n",
    "        self.network = nn.Sequential(*network)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        actions = self.network(x)\n",
    "        return actions\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_actions, epsilon):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.num_actions = num_actions\n",
    "        self.network = DQN(in_channels, num_actions)\n",
    "        \n",
    "        self.eps = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        actions = self.network(x)\n",
    "        return actions\n",
    "    \n",
    "    def e_greedy(self, x):\n",
    "        \n",
    "        actions = self.forward(x)\n",
    "        \n",
    "        greedy = torch.rand(1)\n",
    "        if self.eps < greedy:\n",
    "            return torch.argmax(actions)\n",
    "        else:\n",
    "            return (torch.rand(1) * self.num_actions).type('torch.LongTensor')[0] \n",
    "        \n",
    "    def greedy(self, x):\n",
    "        actions = self.forward(x)\n",
    "        return torch.argmax(actions)\n",
    "    \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.eps = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "        f = open(f\"{self.filename}.csv\", \"w\")\n",
    "        f.close()\n",
    "        \n",
    "    def log(self, msg):\n",
    "        f = open(f\"{self.filename}.csv\", \"a+\")\n",
    "        f.write(f\"{msg}\\n\")\n",
    "        f.close()\n",
    "\n",
    "class Experience_Replay():\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def insert(self, transitions):\n",
    "        \n",
    "        for i in range(len(transitions)):\n",
    "            if len(self.memory) < self.capacity:\n",
    "                self.memory.append(None)\n",
    "            self.memory[self.position] = transitions[i]\n",
    "            self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def get(self, batch_size):\n",
    "        #return random.sample(self.memory, batch_size)\n",
    "        indexes = (np.random.rand(batch_size) * (len(self.memory)-1)).astype(int)\n",
    "        return [self.memory[i] for i in indexes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class Env_Runner:\n",
    "    \n",
    "    def __init__(self, env, agent):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.logger = Logger(\"training_info\")\n",
    "        self.logger.log(\"training_step, return\")\n",
    "        \n",
    "        self.ob = self.env.reset()\n",
    "        self.total_steps = 0\n",
    "        \n",
    "    def run(self, steps):\n",
    "        \n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        \n",
    "        for step in range(steps):\n",
    "            \n",
    "            self.ob = torch.tensor(self.ob) # uint8\n",
    "            action = self.agent.e_greedy(\n",
    "                self.ob.to(device).to(dtype).unsqueeze(0) / 255) # float32+norm\n",
    "            action = action.detach().cpu().numpy()\n",
    "            \n",
    "            obs.append(self.ob)\n",
    "            actions.append(action)\n",
    "            \n",
    "            self.ob, r, done, info, additional_done = self.env.step(action)\n",
    "               \n",
    "            if done: # real environment reset, other add_dones are for q learning purposes\n",
    "                self.ob = self.env.reset()\n",
    "                if \"return\" in info:\n",
    "                    self.logger.log(f'{self.total_steps+step},{info[\"return\"]}')\n",
    "            \n",
    "            rewards.append(r)\n",
    "            dones.append(done or additional_done)\n",
    "            \n",
    "        self.total_steps += steps\n",
    "                                    \n",
    "        return obs, actions, rewards, dones\n",
    "    \n",
    "def make_transitions(obs, actions, rewards, dones):\n",
    "    # observations are in uint8 format\n",
    "    \n",
    "    tuples = []\n",
    "\n",
    "    steps = len(obs) - 1\n",
    "    for t in range(steps):\n",
    "        tuples.append((obs[t],\n",
    "                       actions[t],\n",
    "                       rewards[t],\n",
    "                       obs[t+1],\n",
    "                       int(not dones[t])))\n",
    "        \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39m# loss\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     loss \u001b[39m=\u001b[39m huber_loss(obs_Q, target) \u001b[39m# torch.mean(torch.pow(obs_Q - target, 2))\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    102\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    104\u001b[0m num_model_updates \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/AI/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/AI/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#env_name = 'BreakoutNoFrameskip-v4'\n",
    "#env_name = 'PongNoFrameskip-v4'\n",
    "env_name = 'SpaceInvadersNoFrameskip-v4'\n",
    "\n",
    "# hyperparameter\n",
    "\n",
    "num_stacked_frames = 4\n",
    "\n",
    "replay_memory_size = 250000\n",
    "min_replay_size_to_update = 25000\n",
    "\n",
    "lr = 6e-5 # SpaceInvaders #1e-4 for PONG | 2.5e-5 for Breakout\n",
    "gamma = 0.99\n",
    "minibatch_size = 32\n",
    "steps_rollout = 16\n",
    "\n",
    "start_eps = 1\n",
    "final_eps = 0.1\n",
    "\n",
    "final_eps_frame = 1000000\n",
    "total_steps = 20000000\n",
    "\n",
    "target_net_update = 625 # 10000 steps\n",
    "\n",
    "save_model_steps = 500000\n",
    "\n",
    "# init\n",
    "raw_env = gym.make(env_name)\n",
    "env = Atari_Wrapper(raw_env, env_name, num_stacked_frames, use_add_done=True)\n",
    "\n",
    "in_channels = num_stacked_frames\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "eps_interval = start_eps-final_eps\n",
    "\n",
    "agent = Agent(in_channels, num_actions, start_eps).to(device)\n",
    "target_agent = Agent(in_channels, num_actions, start_eps).to(device)\n",
    "target_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "replay = Experience_Replay(replay_memory_size)\n",
    "runner = Env_Runner(env, agent)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr) #optim.RMSprop(agent.parameters(), lr=lr)\n",
    "huber_loss = torch.nn.SmoothL1Loss()\n",
    "\n",
    "num_steps = 0\n",
    "num_model_updates = 0\n",
    "\n",
    "start_time = time.time()\n",
    "while num_steps < total_steps:\n",
    "    \n",
    "    # set agent exploration | cap exploration after x timesteps to final epsilon\n",
    "    new_epsilon = np.maximum(final_eps, start_eps - ( eps_interval * num_steps/final_eps_frame))\n",
    "    agent.set_epsilon(new_epsilon)\n",
    "    \n",
    "    # get data\n",
    "    obs, actions, rewards, dones = runner.run(steps_rollout)\n",
    "    transitions = make_transitions(obs, actions, rewards, dones)\n",
    "    replay.insert(transitions)\n",
    "    \n",
    "    # add\n",
    "    num_steps += steps_rollout\n",
    "    \n",
    "    # check if update\n",
    "    if num_steps < min_replay_size_to_update:\n",
    "        continue\n",
    "    \n",
    "    # update\n",
    "    for update in range(4):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        minibatch = replay.get(minibatch_size)\n",
    "        \n",
    "        # uint8 to float32 and normalize to 0-1\n",
    "        obs = (torch.stack([i[0] for i in minibatch]).to(device).to(dtype)) / 255 \n",
    "        \n",
    "        actions = np.stack([i[1] for i in minibatch])\n",
    "        rewards = torch.tensor([i[2] for i in minibatch]).to(device)\n",
    "        \n",
    "        # uint8 to float32 and normalize to 0-1\n",
    "        next_obs = (torch.stack([i[3] for i in minibatch]).to(device).to(dtype)) / 255\n",
    "        \n",
    "        dones = torch.tensor([i[4] for i in minibatch]).to(device)\n",
    "        \n",
    "        #  *** double dqn ***\n",
    "        # prediction\n",
    "        \n",
    "        Qs = agent(torch.cat([obs, next_obs]))\n",
    "        obs_Q, next_obs_Q = torch.split(Qs, minibatch_size ,dim=0)\n",
    "        \n",
    "        obs_Q = obs_Q[range(minibatch_size), actions]\n",
    "        \n",
    "        # target\n",
    "        \n",
    "        next_obs_Q_max = torch.max(next_obs_Q,1)[1].detach()\n",
    "        target_Q = target_agent(next_obs)[range(minibatch_size), next_obs_Q_max].detach()\n",
    "        \n",
    "        target = rewards + gamma * target_Q * dones\n",
    "        \n",
    "        # loss\n",
    "        loss = huber_loss(obs_Q, target) # torch.mean(torch.pow(obs_Q - target, 2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    num_model_updates += 1\n",
    "     \n",
    "    # update target network\n",
    "    if num_model_updates%target_net_update == 0:\n",
    "        target_agent.load_state_dict(agent.state_dict())\n",
    "    \n",
    "    # print time\n",
    "    if num_steps%50000 < steps_rollout:\n",
    "        end_time = time.time()\n",
    "        print(f'*** total steps: {num_steps} | time(50K): {end_time - start_time} ***')\n",
    "        start_time = time.time()\n",
    "    \n",
    "    # save the dqn after some time\n",
    "    if num_steps%save_model_steps < steps_rollout:\n",
    "        torch.save(agent,f\"{env_name}-{num_steps}.pt\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch\n",
    "\n",
    "# save agent\n",
    "torch.save(agent,\"agent.pt\")\n",
    "# load agent\n",
    "agent = torch.load(\"agent.pt\")\n",
    "\n",
    "#env = gym.make(env_name)\n",
    "raw_env = gym.make(env_name)\n",
    "env = Atari_Wrapper(raw_env, env_name, num_stacked_frames)\n",
    "\n",
    "steps = 5000\n",
    "ob = env.reset()\n",
    "agent.set_epsilon(0.025)\n",
    "agent.eval()\n",
    "imgs = []\n",
    "for step in range(steps):\n",
    "            \n",
    "    action = agent.e_greedy(torch.tensor(ob, dtype=dtype).unsqueeze(0).to(device) / 255)\n",
    "    action = action.detach().cpu().numpy()\n",
    "    #action = env.action_space.sample()\n",
    "\n",
    "    env.render()\n",
    "    ob, _, done, info, _ = env.step(action)\n",
    "    \n",
    "    time.sleep(0.016)        \n",
    "    if done:\n",
    "        ob = env.reset()\n",
    "        print(info)\n",
    "    \n",
    "    imgs.append(ob)\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
