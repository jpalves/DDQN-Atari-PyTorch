{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "#from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari_Wrapper(gym.Wrapper):\n",
    "    # env wrapper to resize images, grey scale and frame stacking and other misc.\n",
    "    \n",
    "    def __init__(self, env, env_name, k, dsize=(84,84), use_add_done=False):\n",
    "        super(Atari_Wrapper, self).__init__(env)\n",
    "        self.dsize = dsize\n",
    "        self.k = k\n",
    "        self.use_add_done = use_add_done\n",
    "        \n",
    "        # set image cutout depending on game\n",
    "        if \"Pong\" in env_name:\n",
    "            self.frame_cutout_h = (33,-15)\n",
    "            self.frame_cutout_w = (0,-1)\n",
    "        elif \"Breakout\" in env_name:\n",
    "            self.frame_cutout_h = (31,-16)\n",
    "            self.frame_cutout_w = (7,-7)\n",
    "        elif \"SpaceInvaders\" in env_name:\n",
    "            self.frame_cutout_h = (25,-7)\n",
    "            self.frame_cutout_w = (7,-7)\n",
    "        else:\n",
    "            # no cutout\n",
    "            self.frame_cutout_h = (0,-1)\n",
    "            self.frame_cutout_w = (0,-1)\n",
    "        \n",
    "    def reset(self):\n",
    "    \n",
    "        self.Return = 0\n",
    "        self.last_life_count = 0\n",
    "        \n",
    "        ob = self.env.reset()[0]\n",
    "        ob = self.preprocess_observation(ob)\n",
    "        \n",
    "        # stack k times the reset ob\n",
    "        self.frame_stack = np.stack([ob for i in range(self.k)])\n",
    "        \n",
    "        return self.frame_stack\n",
    "    \n",
    "    \n",
    "    def step(self, action): \n",
    "        # do k frameskips, same action for every intermediate frame\n",
    "        # stacking k frames\n",
    "        \n",
    "        reward = 0\n",
    "        done = False\n",
    "        additional_done = False\n",
    "        \n",
    "        # k frame skips or end of episode\n",
    "        frames = []\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            ob, r, d, _,info = self.env.step(action)\n",
    "            \n",
    "            # insert a (additional) done, when agent loses a life (Games with lives)\n",
    "            if self.use_add_done:\n",
    "                if info['lives'] < self.last_life_count:\n",
    "                    additional_done = True  \n",
    "                self.last_life_count = info['lives']\n",
    "            \n",
    "            ob = self.preprocess_observation(ob)\n",
    "            frames.append(ob)\n",
    "            \n",
    "            # add reward\n",
    "            reward += r\n",
    "            \n",
    "            if d: # env done\n",
    "                done = True\n",
    "                break\n",
    "                       \n",
    "        # build the observation\n",
    "        self.step_frame_stack(frames)\n",
    "        \n",
    "        # add info, get return of the completed episode\n",
    "        self.Return += reward\n",
    "        if done:\n",
    "            info[\"return\"] = self.Return\n",
    "            \n",
    "        # clip reward\n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward == 0:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        return self.frame_stack, reward, done, info, additional_done\n",
    "    \n",
    "    def step_frame_stack(self, frames):\n",
    "        \n",
    "        num_frames = len(frames)\n",
    "        \n",
    "        if num_frames == self.k:\n",
    "            self.frame_stack = np.stack(frames)\n",
    "        elif num_frames > self.k:\n",
    "            self.frame_stack = np.array(frames[-k::])\n",
    "        else: # mostly used when episode ends \n",
    "            \n",
    "            # shift the existing frames in the framestack to the front=0 (0->k, index is time)\n",
    "            self.frame_stack[0: self.k - num_frames] = self.frame_stack[num_frames::]\n",
    "            # insert the new frames into the stack\n",
    "            self.frame_stack[self.k - num_frames::] = np.array(frames)  \n",
    "            \n",
    "    def preprocess_observation(self, ob):\n",
    "    # resize and grey and cutout image\n",
    "    \n",
    "        ob = cv2.cvtColor(ob[self.frame_cutout_h[0]:self.frame_cutout_h[1],\n",
    "                           self.frame_cutout_w[0]:self.frame_cutout_w[1]], cv2.COLOR_BGR2GRAY)\n",
    "        ob = cv2.resize(ob, dsize=self.dsize)\n",
    "    \n",
    "        return ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    # nature paper architecture\n",
    "    \n",
    "    def __init__(self, in_channels, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        network = [\n",
    "            torch.nn.Conv2d(in_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        ]\n",
    "        \n",
    "        self.network = nn.Sequential(*network)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        actions = self.network(x)\n",
    "        return actions\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_actions, epsilon):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.num_actions = num_actions\n",
    "        self.network = DQN(in_channels, num_actions)\n",
    "        \n",
    "        self.eps = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        actions = self.network(x)\n",
    "        return actions\n",
    "    \n",
    "    def e_greedy(self, x):\n",
    "        \n",
    "        actions = self.forward(x)\n",
    "        \n",
    "        greedy = torch.rand(1)\n",
    "        if self.eps < greedy:\n",
    "            return torch.argmax(actions)\n",
    "        else:\n",
    "            return (torch.rand(1) * self.num_actions).type('torch.LongTensor')[0] \n",
    "        \n",
    "    def greedy(self, x):\n",
    "        actions = self.forward(x)\n",
    "        return torch.argmax(actions)\n",
    "    \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.eps = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "        f = open(f\"{self.filename}.csv\", \"w\")\n",
    "        f.close()\n",
    "        \n",
    "    def log(self, msg):\n",
    "        f = open(f\"{self.filename}.csv\", \"a+\")\n",
    "        f.write(f\"{msg}\\n\")\n",
    "        f.close()\n",
    "\n",
    "class Experience_Replay():\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def insert(self, transitions):\n",
    "        \n",
    "        for i in range(len(transitions)):\n",
    "            if len(self.memory) < self.capacity:\n",
    "                self.memory.append(None)\n",
    "            self.memory[self.position] = transitions[i]\n",
    "            self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def get(self, batch_size):\n",
    "        #return random.sample(self.memory, batch_size)\n",
    "        indexes = (np.random.rand(batch_size) * (len(self.memory)-1)).astype(int)\n",
    "        return [self.memory[i] for i in indexes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class Env_Runner:\n",
    "    \n",
    "    def __init__(self, env, agent):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.logger = Logger(\"training_info\")\n",
    "        self.logger.log(\"training_step, return\")\n",
    "        \n",
    "        self.ob = self.env.reset()\n",
    "        self.total_steps = 0\n",
    "        \n",
    "    def run(self, steps):\n",
    "        \n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        \n",
    "        for step in range(steps):\n",
    "            \n",
    "            self.ob = torch.tensor(self.ob) # uint8\n",
    "            action = self.agent.e_greedy(\n",
    "                self.ob.to(device).to(dtype).unsqueeze(0) / 255) # float32+norm\n",
    "            action = action.detach().cpu().numpy()\n",
    "            \n",
    "            obs.append(self.ob)\n",
    "            actions.append(action)\n",
    "            \n",
    "            self.ob, r, done, info, additional_done = self.env.step(action)\n",
    "               \n",
    "            if done: # real environment reset, other add_dones are for q learning purposes\n",
    "                self.ob = self.env.reset()\n",
    "                if \"return\" in info:\n",
    "                    self.logger.log(f'{self.total_steps+step},{info[\"return\"]}')\n",
    "            \n",
    "            rewards.append(r)\n",
    "            dones.append(done or additional_done)\n",
    "            \n",
    "        self.total_steps += steps\n",
    "                                    \n",
    "        return obs, actions, rewards, dones\n",
    "    \n",
    "def make_transitions(obs, actions, rewards, dones):\n",
    "    # observations are in uint8 format\n",
    "    \n",
    "    tuples = []\n",
    "\n",
    "    steps = len(obs) - 1\n",
    "    for t in range(steps):\n",
    "        tuples.append((obs[t],\n",
    "                       actions[t],\n",
    "                       rewards[t],\n",
    "                       obs[t+1],\n",
    "                       int(not dones[t])))\n",
    "        \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** total steps: 50000 | time(50K): 87.13653063774109 ***\n",
      "*** total steps: 100000 | time(50K): 112.29230642318726 ***\n",
      "*** total steps: 150000 | time(50K): 112.30644798278809 ***\n",
      "*** total steps: 200000 | time(50K): 111.86752796173096 ***\n",
      "*** total steps: 250000 | time(50K): 112.68673658370972 ***\n",
      "*** total steps: 300000 | time(50K): 113.28925704956055 ***\n",
      "*** total steps: 350000 | time(50K): 113.10483121871948 ***\n",
      "*** total steps: 400000 | time(50K): 112.74963712692261 ***\n",
      "*** total steps: 450000 | time(50K): 112.22463822364807 ***\n",
      "*** total steps: 500000 | time(50K): 112.95104026794434 ***\n",
      "*** total steps: 550000 | time(50K): 112.64524364471436 ***\n",
      "*** total steps: 600000 | time(50K): 112.02221536636353 ***\n",
      "*** total steps: 650000 | time(50K): 112.45492434501648 ***\n",
      "*** total steps: 700000 | time(50K): 113.22197461128235 ***\n",
      "*** total steps: 750000 | time(50K): 112.35356187820435 ***\n",
      "*** total steps: 800000 | time(50K): 111.85386967658997 ***\n",
      "*** total steps: 850000 | time(50K): 112.79671549797058 ***\n",
      "*** total steps: 900000 | time(50K): 112.0091814994812 ***\n",
      "*** total steps: 950000 | time(50K): 111.85113334655762 ***\n",
      "*** total steps: 1000000 | time(50K): 112.23377418518066 ***\n",
      "*** total steps: 1050000 | time(50K): 111.8787453174591 ***\n",
      "*** total steps: 1100000 | time(50K): 112.53483414649963 ***\n",
      "*** total steps: 1150000 | time(50K): 112.01624250411987 ***\n",
      "*** total steps: 1200000 | time(50K): 111.75318384170532 ***\n",
      "*** total steps: 1250000 | time(50K): 112.22471690177917 ***\n",
      "*** total steps: 1300000 | time(50K): 111.98156595230103 ***\n",
      "*** total steps: 1350000 | time(50K): 111.20063734054565 ***\n",
      "*** total steps: 1400000 | time(50K): 111.71076822280884 ***\n",
      "*** total steps: 1450000 | time(50K): 112.43690729141235 ***\n",
      "*** total steps: 1500000 | time(50K): 111.57091498374939 ***\n",
      "*** total steps: 1550000 | time(50K): 111.32093906402588 ***\n",
      "*** total steps: 1600000 | time(50K): 113.21343898773193 ***\n",
      "*** total steps: 1650000 | time(50K): 112.09493851661682 ***\n",
      "*** total steps: 1700000 | time(50K): 111.91460824012756 ***\n",
      "*** total steps: 1750000 | time(50K): 111.5977954864502 ***\n",
      "*** total steps: 1800000 | time(50K): 112.6438193321228 ***\n",
      "*** total steps: 1850000 | time(50K): 112.05165076255798 ***\n",
      "*** total steps: 1900000 | time(50K): 112.39236068725586 ***\n",
      "*** total steps: 1950000 | time(50K): 111.32465434074402 ***\n",
      "*** total steps: 2000000 | time(50K): 113.17941474914551 ***\n",
      "*** total steps: 2050000 | time(50K): 112.31548357009888 ***\n",
      "*** total steps: 2100000 | time(50K): 111.97334408760071 ***\n",
      "*** total steps: 2150000 | time(50K): 111.53163361549377 ***\n",
      "*** total steps: 2200000 | time(50K): 112.4304416179657 ***\n",
      "*** total steps: 2250000 | time(50K): 111.7720742225647 ***\n",
      "*** total steps: 2300000 | time(50K): 112.0833489894867 ***\n",
      "*** total steps: 2350000 | time(50K): 112.72415375709534 ***\n",
      "*** total steps: 2400000 | time(50K): 112.53492331504822 ***\n",
      "*** total steps: 2450000 | time(50K): 111.58129334449768 ***\n",
      "*** total steps: 2500000 | time(50K): 112.01492166519165 ***\n",
      "*** total steps: 2550000 | time(50K): 111.59296655654907 ***\n",
      "*** total steps: 2600000 | time(50K): 112.00762748718262 ***\n",
      "*** total steps: 2650000 | time(50K): 111.98546957969666 ***\n",
      "*** total steps: 2700000 | time(50K): 111.93282413482666 ***\n",
      "*** total steps: 2750000 | time(50K): 112.81473517417908 ***\n",
      "*** total steps: 2800000 | time(50K): 111.92654299736023 ***\n",
      "*** total steps: 2850000 | time(50K): 112.65020370483398 ***\n",
      "*** total steps: 2900000 | time(50K): 111.97633719444275 ***\n",
      "*** total steps: 2950000 | time(50K): 112.61595487594604 ***\n",
      "*** total steps: 3000000 | time(50K): 111.79771375656128 ***\n",
      "*** total steps: 3050000 | time(50K): 111.74970436096191 ***\n",
      "*** total steps: 3100000 | time(50K): 112.16618013381958 ***\n",
      "*** total steps: 3150000 | time(50K): 113.05969786643982 ***\n",
      "*** total steps: 3200000 | time(50K): 111.61382937431335 ***\n",
      "*** total steps: 3250000 | time(50K): 112.45643615722656 ***\n",
      "*** total steps: 3300000 | time(50K): 112.79386019706726 ***\n",
      "*** total steps: 3350000 | time(50K): 111.57831501960754 ***\n",
      "*** total steps: 3400000 | time(50K): 111.91115880012512 ***\n",
      "*** total steps: 3450000 | time(50K): 113.3146436214447 ***\n",
      "*** total steps: 3500000 | time(50K): 113.9719705581665 ***\n",
      "*** total steps: 3550000 | time(50K): 115.5664176940918 ***\n",
      "*** total steps: 3600000 | time(50K): 111.80037546157837 ***\n",
      "*** total steps: 3650000 | time(50K): 113.20929408073425 ***\n",
      "*** total steps: 3700000 | time(50K): 111.5119354724884 ***\n",
      "*** total steps: 3750000 | time(50K): 110.92210507392883 ***\n",
      "*** total steps: 3800000 | time(50K): 110.28395009040833 ***\n",
      "*** total steps: 3850000 | time(50K): 112.05227541923523 ***\n",
      "*** total steps: 3900000 | time(50K): 111.48266053199768 ***\n",
      "*** total steps: 3950000 | time(50K): 111.39324808120728 ***\n",
      "*** total steps: 4000000 | time(50K): 111.10428094863892 ***\n",
      "*** total steps: 4050000 | time(50K): 111.78514337539673 ***\n",
      "*** total steps: 4100000 | time(50K): 111.29699277877808 ***\n",
      "*** total steps: 4150000 | time(50K): 110.20376682281494 ***\n",
      "*** total steps: 4200000 | time(50K): 111.86125612258911 ***\n",
      "*** total steps: 4250000 | time(50K): 111.11977958679199 ***\n",
      "*** total steps: 4300000 | time(50K): 110.89296388626099 ***\n",
      "*** total steps: 4350000 | time(50K): 111.0111198425293 ***\n",
      "*** total steps: 4400000 | time(50K): 111.08540511131287 ***\n",
      "*** total steps: 4450000 | time(50K): 111.25342869758606 ***\n",
      "*** total steps: 4500000 | time(50K): 113.23594188690186 ***\n",
      "*** total steps: 4550000 | time(50K): 113.82577586174011 ***\n",
      "*** total steps: 4600000 | time(50K): 114.08648133277893 ***\n",
      "*** total steps: 4650000 | time(50K): 115.89084696769714 ***\n",
      "*** total steps: 4700000 | time(50K): 115.15734457969666 ***\n",
      "*** total steps: 4750000 | time(50K): 114.40209984779358 ***\n",
      "*** total steps: 4800000 | time(50K): 116.42946553230286 ***\n",
      "*** total steps: 4850000 | time(50K): 118.3664321899414 ***\n",
      "*** total steps: 4900000 | time(50K): 113.42000675201416 ***\n",
      "*** total steps: 4950000 | time(50K): 113.76343107223511 ***\n",
      "*** total steps: 5000000 | time(50K): 112.87803244590759 ***\n",
      "*** total steps: 5050000 | time(50K): 114.41965818405151 ***\n",
      "*** total steps: 5100000 | time(50K): 114.00260925292969 ***\n",
      "*** total steps: 5150000 | time(50K): 114.09179258346558 ***\n",
      "*** total steps: 5200000 | time(50K): 114.93293905258179 ***\n",
      "*** total steps: 5250000 | time(50K): 114.64885067939758 ***\n",
      "*** total steps: 5300000 | time(50K): 114.39094686508179 ***\n",
      "*** total steps: 5350000 | time(50K): 114.42722058296204 ***\n",
      "*** total steps: 5400000 | time(50K): 112.87016081809998 ***\n",
      "*** total steps: 5450000 | time(50K): 115.18303275108337 ***\n",
      "*** total steps: 5500000 | time(50K): 114.03169131278992 ***\n",
      "*** total steps: 5550000 | time(50K): 113.63486433029175 ***\n",
      "*** total steps: 5600000 | time(50K): 114.53212857246399 ***\n",
      "*** total steps: 5650000 | time(50K): 121.27048802375793 ***\n",
      "*** total steps: 5700000 | time(50K): 118.97037720680237 ***\n",
      "*** total steps: 5750000 | time(50K): 117.00960445404053 ***\n",
      "*** total steps: 5800000 | time(50K): 116.04109621047974 ***\n",
      "*** total steps: 5850000 | time(50K): 120.70906019210815 ***\n",
      "*** total steps: 5900000 | time(50K): 118.88364672660828 ***\n",
      "*** total steps: 5950000 | time(50K): 117.43086814880371 ***\n",
      "*** total steps: 6000000 | time(50K): 117.13428449630737 ***\n",
      "*** total steps: 6050000 | time(50K): 117.88709163665771 ***\n",
      "*** total steps: 6100000 | time(50K): 118.1680691242218 ***\n",
      "*** total steps: 6150000 | time(50K): 115.45729541778564 ***\n",
      "*** total steps: 6200000 | time(50K): 116.17840933799744 ***\n",
      "*** total steps: 6250000 | time(50K): 114.84859585762024 ***\n",
      "*** total steps: 6300000 | time(50K): 114.71951127052307 ***\n",
      "*** total steps: 6350000 | time(50K): 114.24379587173462 ***\n",
      "*** total steps: 6400000 | time(50K): 116.25546383857727 ***\n",
      "*** total steps: 6450000 | time(50K): 117.68208599090576 ***\n",
      "*** total steps: 6500000 | time(50K): 119.07054591178894 ***\n",
      "*** total steps: 6550000 | time(50K): 118.14540123939514 ***\n",
      "*** total steps: 6600000 | time(50K): 117.51750159263611 ***\n",
      "*** total steps: 6650000 | time(50K): 117.8677077293396 ***\n",
      "*** total steps: 6700000 | time(50K): 118.1883499622345 ***\n",
      "*** total steps: 6750000 | time(50K): 118.0220639705658 ***\n",
      "*** total steps: 6800000 | time(50K): 118.06282424926758 ***\n",
      "*** total steps: 6850000 | time(50K): 117.17577910423279 ***\n",
      "*** total steps: 6900000 | time(50K): 117.86927127838135 ***\n",
      "*** total steps: 6950000 | time(50K): 118.1480164527893 ***\n",
      "*** total steps: 7000000 | time(50K): 118.37474822998047 ***\n",
      "*** total steps: 7050000 | time(50K): 117.94379782676697 ***\n",
      "*** total steps: 7100000 | time(50K): 116.59999752044678 ***\n",
      "*** total steps: 7150000 | time(50K): 116.94651770591736 ***\n",
      "*** total steps: 7200000 | time(50K): 119.87842631340027 ***\n",
      "*** total steps: 7250000 | time(50K): 118.96880435943604 ***\n",
      "*** total steps: 7300000 | time(50K): 117.58881950378418 ***\n",
      "*** total steps: 7350000 | time(50K): 120.1320571899414 ***\n",
      "*** total steps: 7400000 | time(50K): 120.52945899963379 ***\n",
      "*** total steps: 7450000 | time(50K): 119.66575503349304 ***\n",
      "*** total steps: 7500000 | time(50K): 119.81790471076965 ***\n",
      "*** total steps: 7550000 | time(50K): 120.81782484054565 ***\n",
      "*** total steps: 7600000 | time(50K): 120.32780361175537 ***\n",
      "*** total steps: 7650000 | time(50K): 121.01676154136658 ***\n",
      "*** total steps: 7700000 | time(50K): 122.11284518241882 ***\n",
      "*** total steps: 7750000 | time(50K): 121.11522507667542 ***\n",
      "*** total steps: 7800000 | time(50K): 121.00363802909851 ***\n",
      "*** total steps: 7850000 | time(50K): 121.62592029571533 ***\n",
      "*** total steps: 7900000 | time(50K): 119.26339817047119 ***\n",
      "*** total steps: 7950000 | time(50K): 116.603191614151 ***\n",
      "*** total steps: 8000000 | time(50K): 116.6344199180603 ***\n",
      "*** total steps: 8050000 | time(50K): 116.84021759033203 ***\n",
      "*** total steps: 8100000 | time(50K): 116.91431641578674 ***\n",
      "*** total steps: 8150000 | time(50K): 116.80597853660583 ***\n",
      "*** total steps: 8200000 | time(50K): 117.35644865036011 ***\n",
      "*** total steps: 8250000 | time(50K): 117.328293800354 ***\n",
      "*** total steps: 8300000 | time(50K): 117.52688813209534 ***\n",
      "*** total steps: 8350000 | time(50K): 116.91396999359131 ***\n",
      "*** total steps: 8400000 | time(50K): 118.04375767707825 ***\n",
      "*** total steps: 8450000 | time(50K): 117.2510724067688 ***\n",
      "*** total steps: 8500000 | time(50K): 116.64765429496765 ***\n",
      "*** total steps: 8550000 | time(50K): 116.96152329444885 ***\n",
      "*** total steps: 8600000 | time(50K): 117.13664627075195 ***\n",
      "*** total steps: 8650000 | time(50K): 117.08227300643921 ***\n",
      "*** total steps: 8700000 | time(50K): 117.94397282600403 ***\n",
      "*** total steps: 8750000 | time(50K): 117.29868221282959 ***\n",
      "*** total steps: 8800000 | time(50K): 117.72878336906433 ***\n",
      "*** total steps: 8850000 | time(50K): 116.93745446205139 ***\n",
      "*** total steps: 8900000 | time(50K): 117.62176394462585 ***\n",
      "*** total steps: 8950000 | time(50K): 120.06785583496094 ***\n",
      "*** total steps: 9000000 | time(50K): 119.09568524360657 ***\n",
      "*** total steps: 9050000 | time(50K): 120.736887216568 ***\n",
      "*** total steps: 9100000 | time(50K): 119.85040974617004 ***\n",
      "*** total steps: 9150000 | time(50K): 117.75849723815918 ***\n",
      "*** total steps: 9200000 | time(50K): 117.59769940376282 ***\n",
      "*** total steps: 9250000 | time(50K): 118.3957314491272 ***\n",
      "*** total steps: 9300000 | time(50K): 119.53218364715576 ***\n",
      "*** total steps: 9350000 | time(50K): 119.34371018409729 ***\n",
      "*** total steps: 9400000 | time(50K): 120.3877444267273 ***\n",
      "*** total steps: 9450000 | time(50K): 118.43552231788635 ***\n",
      "*** total steps: 9500000 | time(50K): 121.02100324630737 ***\n",
      "*** total steps: 9550000 | time(50K): 117.76834464073181 ***\n",
      "*** total steps: 9600000 | time(50K): 117.01853680610657 ***\n",
      "*** total steps: 9650000 | time(50K): 116.18702054023743 ***\n",
      "*** total steps: 9700000 | time(50K): 117.29549980163574 ***\n",
      "*** total steps: 9750000 | time(50K): 116.61153507232666 ***\n",
      "*** total steps: 9800000 | time(50K): 116.60955381393433 ***\n",
      "*** total steps: 9850000 | time(50K): 116.89160680770874 ***\n",
      "*** total steps: 9900000 | time(50K): 116.71215987205505 ***\n",
      "*** total steps: 9950000 | time(50K): 116.08934378623962 ***\n",
      "*** total steps: 10000000 | time(50K): 117.14426302909851 ***\n",
      "*** total steps: 10050000 | time(50K): 116.72145795822144 ***\n",
      "*** total steps: 10100000 | time(50K): 116.16811108589172 ***\n",
      "*** total steps: 10150000 | time(50K): 116.94755244255066 ***\n",
      "*** total steps: 10200000 | time(50K): 116.74815225601196 ***\n",
      "*** total steps: 10250000 | time(50K): 115.87580227851868 ***\n",
      "*** total steps: 10300000 | time(50K): 117.04343008995056 ***\n",
      "*** total steps: 10350000 | time(50K): 116.57709550857544 ***\n",
      "*** total steps: 10400000 | time(50K): 116.03326892852783 ***\n",
      "*** total steps: 10450000 | time(50K): 116.66875624656677 ***\n",
      "*** total steps: 10500000 | time(50K): 118.46966218948364 ***\n",
      "*** total steps: 10550000 | time(50K): 116.91325950622559 ***\n",
      "*** total steps: 10600000 | time(50K): 116.4889624118805 ***\n",
      "*** total steps: 10650000 | time(50K): 116.19149136543274 ***\n",
      "*** total steps: 10700000 | time(50K): 117.97666239738464 ***\n",
      "*** total steps: 10750000 | time(50K): 116.29280018806458 ***\n",
      "*** total steps: 10800000 | time(50K): 116.94709706306458 ***\n",
      "*** total steps: 10850000 | time(50K): 116.68108582496643 ***\n",
      "*** total steps: 10900000 | time(50K): 116.85814094543457 ***\n",
      "*** total steps: 10950000 | time(50K): 117.13704490661621 ***\n",
      "*** total steps: 11000000 | time(50K): 116.41558647155762 ***\n",
      "*** total steps: 11050000 | time(50K): 116.92920398712158 ***\n",
      "*** total steps: 11100000 | time(50K): 116.29561257362366 ***\n",
      "*** total steps: 11150000 | time(50K): 116.90495753288269 ***\n",
      "*** total steps: 11200000 | time(50K): 116.99800443649292 ***\n",
      "*** total steps: 11250000 | time(50K): 116.85372543334961 ***\n",
      "*** total steps: 11300000 | time(50K): 117.31505012512207 ***\n",
      "*** total steps: 11350000 | time(50K): 117.5724310874939 ***\n",
      "*** total steps: 11400000 | time(50K): 117.52703094482422 ***\n",
      "*** total steps: 11450000 | time(50K): 117.40067887306213 ***\n",
      "*** total steps: 11500000 | time(50K): 117.23605871200562 ***\n",
      "*** total steps: 11550000 | time(50K): 117.04321432113647 ***\n",
      "*** total steps: 11600000 | time(50K): 118.4578766822815 ***\n",
      "*** total steps: 11650000 | time(50K): 117.08836674690247 ***\n",
      "*** total steps: 11700000 | time(50K): 117.83429455757141 ***\n",
      "*** total steps: 11750000 | time(50K): 117.9163646697998 ***\n",
      "*** total steps: 11800000 | time(50K): 119.03528356552124 ***\n",
      "*** total steps: 11850000 | time(50K): 119.6004638671875 ***\n",
      "*** total steps: 11900000 | time(50K): 116.24529004096985 ***\n",
      "*** total steps: 11950000 | time(50K): 122.15092182159424 ***\n",
      "*** total steps: 12000000 | time(50K): 126.15756559371948 ***\n",
      "*** total steps: 12050000 | time(50K): 120.92951488494873 ***\n",
      "*** total steps: 12100000 | time(50K): 123.32273006439209 ***\n",
      "*** total steps: 12150000 | time(50K): 124.56307935714722 ***\n",
      "*** total steps: 12200000 | time(50K): 125.67191696166992 ***\n",
      "*** total steps: 12250000 | time(50K): 132.7062771320343 ***\n",
      "*** total steps: 12300000 | time(50K): 128.27488040924072 ***\n",
      "*** total steps: 12350000 | time(50K): 127.38303589820862 ***\n",
      "*** total steps: 12400000 | time(50K): 127.95480632781982 ***\n",
      "*** total steps: 12450000 | time(50K): 127.5155725479126 ***\n",
      "*** total steps: 12500000 | time(50K): 128.03910112380981 ***\n",
      "*** total steps: 12550000 | time(50K): 128.46135759353638 ***\n",
      "*** total steps: 12600000 | time(50K): 128.0197012424469 ***\n",
      "*** total steps: 12650000 | time(50K): 128.09409713745117 ***\n",
      "*** total steps: 12700000 | time(50K): 129.0933814048767 ***\n",
      "*** total steps: 12750000 | time(50K): 128.46743202209473 ***\n",
      "*** total steps: 12800000 | time(50K): 127.89420223236084 ***\n",
      "*** total steps: 12850000 | time(50K): 128.0205512046814 ***\n",
      "*** total steps: 12900000 | time(50K): 127.67222237586975 ***\n",
      "*** total steps: 12950000 | time(50K): 128.38199186325073 ***\n",
      "*** total steps: 13000000 | time(50K): 125.0258195400238 ***\n",
      "*** total steps: 13050000 | time(50K): 121.78063941001892 ***\n",
      "*** total steps: 13100000 | time(50K): 121.09285736083984 ***\n",
      "*** total steps: 13150000 | time(50K): 121.2094292640686 ***\n",
      "*** total steps: 13200000 | time(50K): 121.34697270393372 ***\n",
      "*** total steps: 13250000 | time(50K): 121.57297134399414 ***\n",
      "*** total steps: 13300000 | time(50K): 123.72685432434082 ***\n",
      "*** total steps: 13350000 | time(50K): 123.73454475402832 ***\n",
      "*** total steps: 13400000 | time(50K): 122.7268295288086 ***\n",
      "*** total steps: 13450000 | time(50K): 123.75655889511108 ***\n",
      "*** total steps: 13500000 | time(50K): 122.15420842170715 ***\n",
      "*** total steps: 13550000 | time(50K): 126.52082657814026 ***\n",
      "*** total steps: 13600000 | time(50K): 126.16554260253906 ***\n",
      "*** total steps: 13650000 | time(50K): 127.08216261863708 ***\n",
      "*** total steps: 13700000 | time(50K): 124.31765341758728 ***\n",
      "*** total steps: 13750000 | time(50K): 121.15401005744934 ***\n",
      "*** total steps: 13800000 | time(50K): 121.44634771347046 ***\n"
     ]
    }
   ],
   "source": [
    "#env_name = 'BreakoutNoFrameskip-v4'\n",
    "#env_name = 'PongNoFrameskip-v4'\n",
    "env_name = 'SpaceInvadersNoFrameskip-v4'\n",
    "\n",
    "# hyperparameter\n",
    "\n",
    "num_stacked_frames = 4\n",
    "\n",
    "replay_memory_size = 250000\n",
    "min_replay_size_to_update = 25000\n",
    "\n",
    "lr = 6e-5 # SpaceInvaders #1e-4 for PONG | 2.5e-5 for Breakout\n",
    "gamma = 0.99\n",
    "minibatch_size = 32\n",
    "steps_rollout = 16\n",
    "\n",
    "start_eps = 1\n",
    "final_eps = 0.1\n",
    "\n",
    "final_eps_frame = 1000000\n",
    "total_steps = 20000000\n",
    "\n",
    "target_net_update = 625 # 10000 steps\n",
    "\n",
    "save_model_steps = 500000\n",
    "\n",
    "# init\n",
    "raw_env = gym.make(env_name)\n",
    "env = Atari_Wrapper(raw_env, env_name, num_stacked_frames, use_add_done=True)\n",
    "\n",
    "in_channels = num_stacked_frames\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "eps_interval = start_eps-final_eps\n",
    "\n",
    "agent = Agent(in_channels, num_actions, start_eps).to(device)\n",
    "target_agent = Agent(in_channels, num_actions, start_eps).to(device)\n",
    "target_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "replay = Experience_Replay(replay_memory_size)\n",
    "runner = Env_Runner(env, agent)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr) #optim.RMSprop(agent.parameters(), lr=lr)\n",
    "huber_loss = torch.nn.SmoothL1Loss()\n",
    "\n",
    "num_steps = 0\n",
    "num_model_updates = 0\n",
    "\n",
    "start_time = time.time()\n",
    "while num_steps < total_steps:\n",
    "    \n",
    "    # set agent exploration | cap exploration after x timesteps to final epsilon\n",
    "    new_epsilon = np.maximum(final_eps, start_eps - ( eps_interval * num_steps/final_eps_frame))\n",
    "    agent.set_epsilon(new_epsilon)\n",
    "    \n",
    "    # get data\n",
    "    obs, actions, rewards, dones = runner.run(steps_rollout)\n",
    "    transitions = make_transitions(obs, actions, rewards, dones)\n",
    "    replay.insert(transitions)\n",
    "    \n",
    "    # add\n",
    "    num_steps += steps_rollout\n",
    "    \n",
    "    # check if update\n",
    "    if num_steps < min_replay_size_to_update:\n",
    "        continue\n",
    "    \n",
    "    # update\n",
    "    for update in range(4):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        minibatch = replay.get(minibatch_size)\n",
    "        \n",
    "        # uint8 to float32 and normalize to 0-1\n",
    "        obs = (torch.stack([i[0] for i in minibatch]).to(device).to(dtype)) / 255 \n",
    "        \n",
    "        actions = np.stack([i[1] for i in minibatch])\n",
    "        rewards = torch.tensor([i[2] for i in minibatch]).to(device)\n",
    "        \n",
    "        # uint8 to float32 and normalize to 0-1\n",
    "        next_obs = (torch.stack([i[3] for i in minibatch]).to(device).to(dtype)) / 255\n",
    "        \n",
    "        dones = torch.tensor([i[4] for i in minibatch]).to(device)\n",
    "        \n",
    "        #  *** double dqn ***\n",
    "        # prediction\n",
    "        \n",
    "        Qs = agent(torch.cat([obs, next_obs]))\n",
    "        obs_Q, next_obs_Q = torch.split(Qs, minibatch_size ,dim=0)\n",
    "        \n",
    "        obs_Q = obs_Q[range(minibatch_size), actions]\n",
    "        \n",
    "        # target\n",
    "        \n",
    "        next_obs_Q_max = torch.max(next_obs_Q,1)[1].detach()\n",
    "        target_Q = target_agent(next_obs)[range(minibatch_size), next_obs_Q_max].detach()\n",
    "        \n",
    "        target = rewards + gamma * target_Q * dones\n",
    "        \n",
    "        # loss\n",
    "        loss = huber_loss(obs_Q, target) # torch.mean(torch.pow(obs_Q - target, 2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    num_model_updates += 1\n",
    "     \n",
    "    # update target network\n",
    "    if num_model_updates%target_net_update == 0:\n",
    "        target_agent.load_state_dict(agent.state_dict())\n",
    "    \n",
    "    # print time\n",
    "    if num_steps%50000 < steps_rollout:\n",
    "        end_time = time.time()\n",
    "        print(f'*** total steps: {num_steps} | time(50K): {end_time - start_time} ***')\n",
    "        start_time = time.time()\n",
    "    \n",
    "    # save the dqn after some time\n",
    "    if num_steps%save_model_steps < steps_rollout:\n",
    "        torch.save(agent,f\"{env_name}-{num_steps}.pt\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch\n",
    "\n",
    "# save agent\n",
    "torch.save(agent,\"agent.pt\")\n",
    "# load agent\n",
    "agent = torch.load(\"agent.pt\")\n",
    "\n",
    "#env = gym.make(env_name)\n",
    "raw_env = gym.make(env_name)\n",
    "env = Atari_Wrapper(raw_env, env_name, num_stacked_frames)\n",
    "\n",
    "steps = 5000\n",
    "ob = env.reset()\n",
    "agent.set_epsilon(0.025)\n",
    "agent.eval()\n",
    "imgs = []\n",
    "for step in range(steps):\n",
    "            \n",
    "    action = agent.e_greedy(torch.tensor(ob, dtype=dtype).unsqueeze(0).to(device) / 255)\n",
    "    action = action.detach().cpu().numpy()\n",
    "    #action = env.action_space.sample()\n",
    "\n",
    "    env.render()\n",
    "    ob, _, done, info, _ = env.step(action)\n",
    "    \n",
    "    time.sleep(0.016)        \n",
    "    if done:\n",
    "        ob = env.reset()\n",
    "        print(info)\n",
    "    \n",
    "    imgs.append(ob)\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
